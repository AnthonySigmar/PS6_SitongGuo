{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Problem Set 6 - Waze Shiny Dashboard\"\n",
        "author: \"Sitong Guo\"\n",
        "date: today\n",
        "format: \n",
        "  pdf:\n",
        "    include-in-header: \n",
        "       text: |\n",
        "         \\usepackage{fvextra}\n",
        "         \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n",
        "include-before-body:\n",
        "  text: |\n",
        "    \\RecustomVerbatimEnvironment{verbatim}{Verbatim}{\n",
        "      showspaces = false,\n",
        "      showtabs = false,\n",
        "      breaksymbolleft={},\n",
        "      breaklines\n",
        "    }\n",
        "---\n",
        "\n",
        "1. **ps6:** Due Sat 23rd at 5:00PM Central. Worth 100 points (80 points from questions, 10 points for correct submission and 10 points for code style) + 10 extra credit. \n",
        "\n",
        "We use (`*`) to indicate a problem that we think might be time consuming. \n",
        "\n",
        "# Steps to submit (10 points on PS6) {-}\n",
        "\n",
        "1. \"This submission is my work alone and complies with the 30538 integrity\n",
        "policy.\" Add your initials to indicate your agreement: \\*\\*\\_\\_\\*\\*\n",
        "2. \"I have uploaded the names of anyone I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**\"  \\*\\*\\_\\_\\*\\* (2 point)\n",
        "3. Late coins used this pset: \\*\\*\\_\\_\\*\\* Late coins left after submission: \\*\\*\\_\\_\\*\\*\n",
        "\n",
        "4. Before starting the problem set, make sure to read and agree to the terms of data usage for the Waze data [here](https://canvas.uchicago.edu/courses/59054/quizzes/130617).\n",
        "\n",
        "5. Knit your `ps6.qmd` as a pdf document and name it `ps6.pdf`.\n",
        "6. Submit your `ps6.qmd`, `ps6.pdf`, `requirements.txt`, and all created folders (we will create three Shiny apps so you will have at least three additional folders) to the gradescope repo assignment (5 points).\n",
        "7. Submit `ps6.pdf` and also link your Github repo via Gradescope (5 points)\n",
        "8. Tag your submission in Gradescope. For the Code Style part (10 points) please tag the whole correspondingsection for the code style rubric.\n",
        "\n",
        "*Notes: see the [Quarto documentation (link)](https://quarto.org/docs/authoring/figures.html) for directions on inserting images into your knitted document.*\n",
        "\n",
        "*IMPORTANT: For the App portion of the PS, in case you can not arrive to the expected functional dashboard we will need to take a look at your `app.py` file. You can use the following code chunk template to \"import\" and print the content of that file. Please, don't forget to also tag the corresponding code chunk as part of your submission!*\n"
      ],
      "id": "a916c84d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "def print_file_contents(file_path):\n",
        "    \"\"\"Print contents of a file.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            content = f.read()\n",
        "            print(\"```python\")\n",
        "            print(content)\n",
        "            print(\"```\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"```python\")\n",
        "        print(f\"Error: File '{file_path}' not found\")\n",
        "        print(\"```\")\n",
        "    except Exception as e:\n",
        "        print(\"```python\") \n",
        "        print(f\"Error reading file: {e}\")\n",
        "        print(\"```\")\n",
        "\n",
        "print_file_contents(\"./top_alerts_map_byhour/app.py\") # Change accordingly"
      ],
      "id": "9508c335",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "# Import required packages.\n",
        "import pandas as pd\n",
        "import altair as alt \n",
        "\n",
        "from datetime import date\n",
        "import numpy as np\n",
        "alt.data_transformers.disable_max_rows() \n",
        "\n",
        "import json"
      ],
      "id": "97f536e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Background {-}\n",
        "\n",
        "## Data Download and Exploration (20 points){-} \n",
        "\n",
        "1. \n"
      ],
      "id": "36a00749"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import zipfile"
      ],
      "id": "ef1cdcab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "zip_path = 'C:/Users/RedthinkerDantler/Documents/GitHub/DPPP2/student30538/problem_sets/ps6/waze_data.zip'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('C:/Users/RedthinkerDantler/Documents/GitHub/DPPP2/student30538/problem_sets/ps6/waze_data')\n",
        "\n",
        "sample_data_path = 'C:/Users/RedthinkerDantler/Documents/GitHub/DPPP2/student30538/problem_sets/ps6/waze_data/waze_data_sample.csv'\n",
        "waze_sample_df = pd.read_csv(sample_data_path)\n",
        "\n",
        "columns_to_exclude = ['ts', 'geo', 'geoWKT']\n",
        "altair_data_types = [(col, \n",
        "                      'Quantitative' if pd.api.types.is_numeric_dtype(waze_sample_df[col]) else 'Nominal') \n",
        "                     for col in waze_sample_df.columns if col not in columns_to_exclude]\n",
        "print(\"Variable Names and Data Types:\")\n",
        "print(altair_data_types)"
      ],
      "id": "140013e0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. \n"
      ],
      "id": "de0b9977"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "full_data_path = 'C:/Users/RedthinkerDantler/Documents/GitHub/DPPP2/student30538/problem_sets/ps6/waze_data/waze_data.csv'\n",
        "waze_full_df = pd.read_csv(full_data_path)\n",
        "\n",
        "missing_data_summary = waze_full_df.isnull().sum().reset_index()\n",
        "missing_data_summary.columns = ['Variable', 'Missing_Count']\n",
        "missing_data_summary['Non_Missing_Count'] = len(waze_full_df) - missing_data_summary['Missing_Count']\n",
        "\n",
        "stacked_data = pd.melt(missing_data_summary, id_vars=['Variable'], \n",
        "                       value_vars=['Missing_Count', 'Non_Missing_Count'], \n",
        "                       var_name='Category', value_name='Count')\n",
        "\n",
        "chart = alt.Chart(stacked_data).mark_bar().encode(\n",
        "    x='Variable:N',\n",
        "    y='Count:Q',\n",
        "    color='Category:N'\n",
        ").properties(title='Missing Values in Variables')\n",
        "chart.show()"
      ],
      "id": "c2ae6b48",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Variables with missing values are: nThumbsUp, street, subtype.\n",
        "Among which the most incomplete variable is nThumbsUp.\n",
        "\n",
        "3. \n",
        "a,"
      ],
      "id": "d260d716"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "unique_types = waze_full_df['type'].unique()\n",
        "unique_subtypes = waze_full_df.groupby('type')['subtype'].unique()\n",
        "print(f\"Unique types: {unique_types}\")\n",
        "print(f\"Unique subtypes: {unique_subtypes}\")\n",
        "\n",
        "# Identify types with NA subtype, check if there are sub-subs\n",
        "na_subtypes = unique_subtypes.apply(lambda x: x[pd.isnull(x)])\n",
        "na_subtypes_count = unique_subtypes.apply(lambda x: pd.isnull(x).any()).sum()\n",
        "print(f\"Number of types that have nan subtype:{na_subtypes_count}\" )"
      ],
      "id": "b51a6a5a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I can identify the types with sub-subs: HAZARD\n",
        "\n",
        "b,"
      ],
      "id": "ba6887f7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Bulleted listed with the values at each layer given this hierarchy.  Readable\n",
        "hierarchy = {}\n",
        "for t, subtypes in unique_subtypes.items():\n",
        "    readable_type = t.replace(\"_\", \" \").title()\n",
        "    readable_subtypes = [\n",
        "        st.replace(\"_\", \" \").title().replace(readable_type, \"\").strip() if pd.notnull(st) else \"Unclassified\"\n",
        "        for st in subtypes\n",
        "    ]\n",
        "    hierarchy[readable_type] = readable_subtypes"
      ],
      "id": "cd0d6cb5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sub_subtype_hierarchy = {\n",
        "    \"Hazard\": {\n",
        "        \"Unclassified\":[],\n",
        "        \"On Road\": [\"Unclassified\", \"Car Stopped\", \"Construction\", \"Emergency Vehicle\", \"Ice\", \"Object\", \"Pot Hole\", \"Traffic Light Fault\", \"Lane Closed\",\"Road Kill\"],\n",
        "        \"On Shoulder\": [\"Unclassified\", \"Car stopped\",\"Animals\", \"Missing Sign\"],\n",
        "        \"Weather\": [\"Unclassified\", \"Flood\",\"Fog\",\"Heavy Snow\",\"Hail\"]\n",
        "    },\n",
        "    \"Accident\": {\n",
        "        \"Unclassified\":[],\n",
        "        \"Major\": [],\n",
        "        \"Minor\": [],\n",
        "    },\n",
        "    \"Jam\": {\n",
        "        \"Unclassified\":[],\n",
        "        \"Heavy Traffic\": [],\n",
        "        \"Moderate Traffic\": [],\n",
        "        \"Stand Still Traffic\": [],\n",
        "        \"Light Traffic\": []\n",
        "    },\n",
        "    \"Road Closed\":{\n",
        "        \"Unclassified\":[],\n",
        "        \"Event\": [],\n",
        "        \"Construction\": [],\n",
        "        \"Hazard\": []\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Full Hierarchy with Sub-Subtypes:\")\n",
        "for t, subtypes in sub_subtype_hierarchy.items():\n",
        "    print(f\"- {t}\")\n",
        "    for subtype, sub_subtypes in subtypes.items():\n",
        "        print(f\"  - {subtype}\")\n",
        "        for sub_subtype in sub_subtypes:\n",
        "            print(f\"    - {sub_subtype}\")"
      ],
      "id": "46759ff2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c,"
      ],
      "id": "2c337983"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Keep NA subtypes"
      ],
      "id": "4590a322",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Yes we should since that they do contain the information of those issues that are not readily classified yet. \n",
        "\n",
        "4. \n",
        "a, b:"
      ],
      "id": "d7aae93d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define the primary subtypes manually\n",
        "primary_subtypes = {\n",
        "    \"Hazard\": [\"On Road\", \"On Shoulder\", \"Weather\", \"Unclassified\"],\n",
        "    \"Accident\": [\"Major\", \"Minor\", \"Unclassified\"],\n",
        "    \"Jam\": [\"Light Traffic\", \"Moderate Traffic\", \"Heavy Traffic\", \"Stand Still Traffic\", \"Unclassified\"],\n",
        "    \"Road Closed\": [\"Event\", \"Construction\", \"Hazard\", \"Unclassified\"]\n",
        "}\n",
        "\n",
        "def clean_type_and_subtypes(row):\n",
        "    # Clean the type \n",
        "    readable_type = row['type'].replace(\"_\", \" \").title()\n",
        "\n",
        "    # Clean the subtype \n",
        "    if pd.notnull(row['subtype']):\n",
        "        readable_subtype = row['subtype'].replace(\"_\", \" \").title().replace(readable_type, \"\").strip()  # Clean the subtype\n",
        "    else:\n",
        "        readable_subtype = \"Unclassified\"\n",
        "\n",
        "    # Initialize subsubtype as None\n",
        "    readable_subsubtype = None\n",
        "\n",
        "    # Check if the cleaned subtype is a sub\n",
        "    if readable_subtype in primary_subtypes.get(readable_type, []):\n",
        "        readable_subsubtype = \"Unclassified\"\n",
        "    else:\n",
        "        readable_subsubtype = readable_subtype\n",
        "\n",
        "        # For those under hazard, assign the proper Subtype in the dict!\n",
        "        for primary in primary_subtypes.get(readable_type, []):\n",
        "            if primary in readable_subtype:\n",
        "                readable_subtype = primary\n",
        "                # Drop the subtype string in the subsub\n",
        "                readable_subsubtype = readable_subsubtype.replace(primary, \"\").strip()\n",
        "                break\n",
        "\n",
        "    return pd.Series([readable_type, readable_subtype, readable_subsubtype])\n",
        "\n",
        "crosswalk_df = waze_full_df[['type', 'subtype']].drop_duplicates()\n",
        "crosswalk_df[['updated_type', 'updated_subtype', 'updated_subsubtype']] = crosswalk_df.apply(clean_type_and_subtypes, axis=1)\n",
        "\n",
        "crosswalk_df"
      ],
      "id": "1b2fb722",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c,"
      ],
      "id": "e984163a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "merged_data = pd.merge(waze_full_df, crosswalk_df, on=['type', 'subtype'], how='left')\n",
        "\n",
        "# Count rows for accident unclassified\n",
        "accident_unclassified_count = merged_data[\n",
        "    (merged_data['updated_type'] == 'Accident') & \n",
        "    (merged_data['updated_subtype'] == 'Unclassified')\n",
        "].shape[0]\n",
        "\n",
        "print(f\"Number of Accident-Unclassified: {accident_unclassified_count}\")"
      ],
      "id": "1b626500",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "d,"
      ],
      "id": "69c11101"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Extract unique combinations of type-subtype from both data\n",
        "crosswalk_combinations = crosswalk_df[['type', 'subtype']].drop_duplicates()\n",
        "merged_combinations = merged_data[['type', 'subtype']].drop_duplicates()\n",
        "\n",
        "# Compare unique combination\n",
        "are_combinations_equal = crosswalk_combinations.equals(merged_combinations)\n",
        "\n",
        "print(f\"Do they have same values in type-subtype? '{are_combinations_equal}'\")"
      ],
      "id": "c4005e99",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# App #1: Top Location by Alert Type Dashboard (30 points){-}\n",
        "\n",
        "1. \n",
        "a. \n",
        "Prompt to ChatGPT:\n",
        "Write a regex pattern to extract latitude and longitude from a string formatted like this: \"POINT (longitude latitude)\".\n",
        "\n",
        "Response:\n",
        "The regex pattern is:\n",
        "\n",
        "python:\n",
        "r\"POINT \\((-?\\d+\\.\\d+) (-?\\d+\\.\\d+)\\)\"\n",
        "Explanation:\n",
        "POINT matches the literal string.\n",
        "\\( and \\) escape parentheses.\n",
        "-? matches an optional negative sign.\n",
        "\\d+ matches one or more digits.\n",
        "\\. matches a literal dot.\n",
        "Captures the first number as longitude and the second as latitude."
      ],
      "id": "6d80f11c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import re"
      ],
      "id": "2d5f69bc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Delete that space after POINT, no spaces between point and (\n",
        "# Extract latitude and longitude\n",
        "merged_data['latitude'] = merged_data['geo'].str.extract(r\"POINT\\((-?\\d+\\.\\d+) (-?\\d+\\.\\d+)\\)\", expand=True)[1].astype(float)\n",
        "merged_data['longitude'] = merged_data['geo'].str.extract(r\"POINT\\((-?\\d+\\.\\d+) (-?\\d+\\.\\d+)\\)\", expand=True)[0].astype(float) \n",
        "merged_data['longitude']"
      ],
      "id": "8961ffb6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b. "
      ],
      "id": "7f28f048"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create bins with step size 0.01\n",
        "merged_data['latitude'] = (merged_data['latitude'] // 0.01 * 0.01).round(2)\n",
        "merged_data['longitude'] = (merged_data['longitude'] // 0.01 * 0.01).round(2)\n",
        "\n",
        "# most frequent bin combination\n",
        "most_frequent_bin = merged_data.groupby(['latitude', 'longitude']).size().idxmax()\n",
        "print(f\"The most frequent bin is: {most_frequent_bin}\")"
      ],
      "id": "08de4cef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c. "
      ],
      "id": "969b7247"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Aggregate data\n",
        "aggregated_df = (\n",
        "    merged_data.groupby(['latitude', 'longitude', 'updated_type', 'updated_subtype'])\n",
        "    .size()\n",
        "    .reset_index(name='alert count')\n",
        ")\n",
        "\n",
        "output_path = \"C:/Users/RedthinkerDantler/Documents/GitHub/DPPP2/PS6_SitongGuo/top_alerts_map/top_alerts_map.csv\"\n",
        "aggregated_df.to_csv(output_path, index=False)\n",
        "\n",
        "# Check aggregation level and rows\n",
        "print(f\"Aggregation level: latitude, longitude, type, subtype. \\nNumber of rows in the DataFrame: {aggregated_df.shape[0]}\")"
      ],
      "id": "b8d9ac34",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. "
      ],
      "id": "9d9c701c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Filter for Jam Heavy Traffic\n",
        "filtered_df = (\n",
        "    aggregated_df[\n",
        "        (aggregated_df['updated_type'] == 'Jam') &\n",
        "        (aggregated_df['updated_subtype'] == 'Heavy Traffic')\n",
        "    ]\n",
        "    .nlargest(10, 'alert count')\n",
        ")\n",
        "\n",
        "# Define axis ranges for better presentation!\n",
        "latitude_min, latitude_max = filtered_df['latitude'].min() - 0.01, filtered_df['latitude'].max() + 0.01\n",
        "longitude_min, longitude_max = filtered_df['longitude'].min() - 0.01, filtered_df['longitude'].max() + 0.01\n",
        "\n",
        "chart = alt.Chart(filtered_df).mark_circle().encode(\n",
        "    x=alt.X('longitude:Q', title='Longitude', scale=alt.Scale(domain=[longitude_min, longitude_max])),\n",
        "    y=alt.Y('latitude:Q', title='Latitude', scale=alt.Scale(domain=[latitude_min, latitude_max])),\n",
        "    size=alt.Size('alert count:Q', title='Alert Count', scale=alt.Scale(range=[50, 1000])),\n",
        "    color=alt.Color('alert count:Q', scale=alt.Scale(scheme='reds')),\n",
        "    tooltip=['latitude:Q', 'longitude:Q', 'alert count:Q']\n",
        ").properties(\n",
        "    title='Top 10 Locales for Jam - Heavy Traffic Alerts',\n",
        "    width=800,\n",
        "    height=600\n",
        ")\n",
        "chart.show()"
      ],
      "id": "5988c742",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. \n",
        "a. "
      ],
      "id": "71d61e42"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "# Download using python\n",
        "url = \"https://data.cityofchicago.org/api/geospatial/bbvz-uum9?method=export&format=GeoJSON\"\n",
        "file_path = \"C:/Users/RedthinkerDantler/Documents/GitHub/DPPP2/PS6_SitongGuo/top_alerts_map/chicago-boundaries.geojson\"\n",
        "\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    print(\"successful!\")\n",
        "else:\n",
        "    print(f\"Failed:{response.status_code}\")"
      ],
      "id": "0c4643d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b. "
      ],
      "id": "0922d385"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "# MODIFY ACCORDINGLY\n",
        "\n",
        "with open(file_path) as f:\n",
        "    chicago_geojson = json.load(f)\n",
        "\n",
        "geo_data = alt.Data(values=chicago_geojson[\"features\"])"
      ],
      "id": "fc842bfe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. \n"
      ],
      "id": "c2342919"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create map layer\n",
        "map_layer = alt.Chart(geo_data).mark_geoshape(\n",
        "    fill=\"lightgray\",  \n",
        "    stroke=\"white\",    \n",
        "    strokeWidth=0.5   \n",
        ").properties(\n",
        "    width=800,  \n",
        "    height=600  \n",
        ").project(\n",
        "    type=\"equirectangular\"  # Projection type\n",
        ")"
      ],
      "id": "ac88b5a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# scatter plot layer\n",
        "scatter_layer = alt.Chart(filtered_df).mark_circle().encode(\n",
        "    \n",
        "    longitude='longitude:Q',\n",
        "    latitude='latitude:Q',\n",
        "    size=alt.Size('alert count:Q', scale=alt.Scale(range=[50, 500])),  \n",
        "    color=alt.Color('alert count:Q', scale=alt.Scale(scheme='reds')),  \n",
        "    tooltip=['latitude', 'longitude', 'alert count']  \n",
        ").properties(\n",
        "    title=\"Top 10 Jam - Heavy Traffic Alerts in Chicago\"\n",
        ")"
      ],
      "id": "dafbcb57",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set axis domains for alignment\n",
        "lat_min, lat_max = filtered_df['latitude'].min() - 0.01, filtered_df['latitude'].max() + 0.01\n",
        "lon_min, lon_max = filtered_df['longitude'].min() - 0.01, filtered_df['longitude'].max() + 0.01\n",
        "\n",
        "# Combine the layers!!\n",
        "combined_plot = (map_layer + scatter_layer).configure_view(\n",
        "    stroke=None   \n",
        "    # Remove the default borders around the map\n",
        ").properties(\n",
        "    width=800,\n",
        "    height=600\n",
        ").configure_title(\n",
        "    fontSize=16,\n",
        "    anchor=\"start\"\n",
        ").encode(\n",
        "    x=alt.X('longitude:Q', scale=alt.Scale(domain=[lon_min, lon_max]), title=\"Longitude\"),\n",
        "    y=alt.Y('latitude:Q', scale=alt.Scale(domain=[lat_min, lat_max]), title=\"Latitude\")\n",
        ")\n",
        "\n",
        "combined_plot"
      ],
      "id": "aba67dc8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. \n",
        "a. "
      ],
      "id": "9540c01f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "![2-5-1](C:/Users/RedthinkerDantler/Documents/GitHub/DPPP2/PS6_SitongGuo/top_alerts_map/2_5_1.png)"
      ],
      "id": "c9b8f65e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are 16 combinations in the menu.\n",
        "\n",
        "b. "
      ],
      "id": "9fbbfc04"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "![2-5-2](C:/Users/RedthinkerDantler/Documents/GitHub/DPPP2/PS6_SitongGuo/top_alerts_map/2_5_2.png)"
      ],
      "id": "39b78e12",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c. "
      ],
      "id": "c3448e9d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "![2-5-3](C:/Users/RedthinkerDantler/Documents/GitHub/DPPP2/PS6_SitongGuo/top_alerts_map/2_5_3.png)"
      ],
      "id": "9960a742",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "filtered_df1 = (\n",
        "    aggregated_df[\n",
        "        (aggregated_df['updated_type'] == 'Road Closed') &\n",
        "        (aggregated_df['updated_subtype'] == 'Event')\n",
        "    ]\n",
        "    .nlargest(10, 'alert count')\n",
        ")\n",
        "\n",
        "scatter_layer = alt.Chart(filtered_df1).mark_circle().encode(\n",
        "    longitude='longitude:Q',\n",
        "    latitude='latitude:Q',\n",
        "    size=alt.Size('alert count:Q', scale=alt.Scale(range=[50, 500])),  \n",
        "    color=alt.Color('alert count:Q', scale=alt.Scale(scheme='reds')),  \n",
        "    tooltip=['latitude', 'longitude', 'alert count']  \n",
        ").properties(\n",
        "    title=\"Top 10 Road Closure - Event Alerts in Chicago\"\n",
        ")\n",
        "\n",
        "lat_min, lat_max = filtered_df1['latitude'].min() - 0.01, filtered_df1['latitude'].max() + 0.01\n",
        "lon_min, lon_max = filtered_df1['longitude'].min() - 0.01, filtered_df1['longitude'].max() + 0.01\n",
        "combined_plot1 = (map_layer + scatter_layer).configure_view(\n",
        "    stroke=None   \n",
        "    # Remove the default borders around the map\n",
        ").properties(\n",
        "    width=800,\n",
        "    height=600\n",
        ").configure_title(\n",
        "    fontSize=16,\n",
        "    anchor=\"start\"\n",
        ").encode(\n",
        "    x=alt.X('longitude:Q', scale=alt.Scale(domain=[lon_min, lon_max]), title=\"Longitude\"),\n",
        "    y=alt.Y('latitude:Q', scale=alt.Scale(domain=[lat_min, lat_max]), title=\"Latitude\")\n",
        ")\n",
        "combined_plot1"
      ],
      "id": "5c81ab3c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the plot, the most common area (latitude, longitude) is (41.96, -87.75) .\n",
        "\n",
        "d. \n",
        "Question: which approximate area is more likely for minor traffic accidents to occur?  "
      ],
      "id": "576876cb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "![2-5-4](C:/Users/RedthinkerDantler/Documents/GitHub/DPPP2/PS6_SitongGuo/top_alerts_map/2_5_4.png)"
      ],
      "id": "f74b46d9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The area is the southeast area of Chicago, judging from the geographical distribution.\n",
        "\n",
        "e. \n",
        "Add the subsubtype column to provide more granular analysis on the hazard subtypes. Or, add the roadtype or the ts columns to examine insights on relationship between alerts and road types or the timing.\n",
        "\n",
        "The code in App1:"
      ],
      "id": "5904c27a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def print_file_contents(file_path):\n",
        "    \"\"\"Print contents of a file.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            content = f.read()\n",
        "            print(\"```python\")\n",
        "            print(content)\n",
        "            print(\"```\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"```python\")\n",
        "        print(f\"Error: File '{file_path}' not found\")\n",
        "        print(\"```\")\n",
        "    except Exception as e:\n",
        "        print(\"```python\") \n",
        "        print(f\"Error reading file: {e}\")\n",
        "        print(\"```\")\n",
        "\n",
        "print_file_contents(\"C:/Users/RedthinkerDantler/Documents/GitHub/DPPP2/PS6_SitongGuo/top_alerts_map/app1/app1.py\") # Change accordingly"
      ],
      "id": "3167de2e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# App #2: Top Location by Alert Type and Hour Dashboard (20 points) {-}\n",
        "\n",
        "1. \n",
        "a. \n",
        "No. The ts column contains highly specified granular data (to minute and second). Collapsing by ts would create numerous unique rows for every specific timestamp as each moment is unique. This would not aggregate data meaningfully and will even increase computational complexity.\n",
        "    \n",
        "b. "
      ],
      "id": "29ad81d5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hour_data = merged_data\n",
        "hour_data['ts'] = pd.to_datetime(hour_data['ts'])\n",
        "\n",
        "hour_data['hour'] = hour_data['ts'].dt.strftime('%H:00')"
      ],
      "id": "e961b5ae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Collapse \n",
        "collapsed_data = (\n",
        "    hour_data.groupby(['hour', 'updated_type', 'updated_subtype', 'longitude', 'latitude'])\n",
        "    .size()\n",
        "    .reset_index(name='alert_count')  \n",
        ")"
      ],
      "id": "8cceb43d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# top 10 locations\n",
        "top_10_data = (\n",
        "    collapsed_data\n",
        "    .sort_values(['hour', 'updated_type', 'updated_subtype', 'alert_count'], ascending=[True, True, True, False])\n",
        "    .groupby(['hour', 'updated_type', 'updated_subtype'])\n",
        "    .head(10)  \n",
        ")\n",
        "\n",
        "top_10_data.to_csv(\"C:/Users/RedthinkerDantler/Documents/GitHub/DPPP2/PS6_SitongGuo/top_alerts_map_byhour/top_alerts_map_byhour.csv\", index=False)\n",
        "\n",
        "print(f\"Number of rows in the collapsed dataset: {len(top_10_data)}\")"
      ],
      "id": "4f8c75d4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c."
      ],
      "id": "aac4e0a7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "heavy_traffic_data = top_10_data[\n",
        "    (top_10_data['updated_type'] == 'Jam') &\n",
        "    (top_10_data['updated_subtype'] == 'Heavy Traffic')\n",
        "]\n",
        "\n",
        "# Select three times of day\n",
        "times = ['08:00', '12:00', '18:00']  \n",
        "\n",
        "# Create a plot for each time\n",
        "plots = []\n",
        "for time in times:\n",
        "    # specific hour\n",
        "    data_for_time = heavy_traffic_data[heavy_traffic_data['hour'] == time]\n",
        "\n",
        "    # scatter plot for top 10 locations\n",
        "    scatter_layer = alt.Chart(data_for_time).mark_circle().encode(\n",
        "        longitude='longitude:Q',\n",
        "        latitude='latitude:Q',\n",
        "        size=alt.Size('alert_count:Q', scale=alt.Scale(range=[50, 500])),  \n",
        "        color=alt.Color('alert_count:Q', scale=alt.Scale(scheme='reds')),  \n",
        "        tooltip=['latitude', 'longitude', 'alert_count']  \n",
        "    ).properties(\n",
        "        title = f\"Top 10 Jam - Heavy Traffic Alerts by {time} in Chicago\"\n",
        "    )\n",
        "\n",
        "    map_layer = map_layer\n",
        "\n",
        "    # Set axis domains for alignment\n",
        "    lat_min, lat_max = data_for_time['latitude'].min() - 0.01, data_for_time['latitude'].max() + 0.01\n",
        "    lon_min, lon_max = data_for_time['longitude'].min() - 0.01, data_for_time['longitude'].max() + 0.01\n",
        "\n",
        "    # Combine the layers!!\n",
        "    plot = (map_layer + scatter_layer).encode(\n",
        "        x=alt.X('longitude:Q', scale=alt.Scale(domain=[lon_min, lon_max]), title=\"Longitude\"),\n",
        "        y=alt.Y('latitude:Q', scale=alt.Scale(domain=[lat_min, lat_max]), title=\"Latitude\")\n",
        "    ).properties(\n",
        "        width=800,\n",
        "        height=600\n",
        "    )\n",
        "    plots.append(plot)\n",
        "\n",
        "# Concatenate the plots vertically\n",
        "final_plot = alt.vconcat(*plots).configure_view(\n",
        "    stroke=None\n",
        ").configure_title(\n",
        "    fontSize=16,\n",
        "    anchor=\"start\"\n",
        ")\n",
        "final_plot"
      ],
      "id": "b5daf2c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.\n",
        "a. "
      ],
      "id": "5c7db2fe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "![3-2-1](C:/Users/RedthinkerDantler/Documents/GitHub/DPPP2/PS6_SitongGuo/top_alerts_map_byhour/3_2_1.png)"
      ],
      "id": "39f4aba1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b. "
      ],
      "id": "222d728e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "![3-2-2: '8:00'](C:/Users/RedthinkerDantler/Documents/GitHub/DPPP2/PS6_SitongGuo/top_alerts_map_byhour/3_2_1.png)\n",
        "![3-2-2: '12:00'](C:/Users/RedthinkerDantler/Documents/GitHub/DPPP2/PS6_SitongGuo/top_alerts_map_byhour/3_2_21.png)\n",
        "![3-2-2: '18:00'](C:/Users/RedthinkerDantler/Documents/GitHub/DPPP2/PS6_SitongGuo/top_alerts_map_byhour/3_2_22.png)"
      ],
      "id": "2e99b0b7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c. "
      ],
      "id": "c3bb28d1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "![3-2-3: 'morning'](C:/Users/RedthinkerDantler/Documents/GitHub/DPPP2/PS6_SitongGuo/top_alerts_map_byhour/3_2_31.png)\n",
        "![3-2-3: 'evening'](C:/Users/RedthinkerDantler/Documents/GitHub/DPPP2/PS6_SitongGuo/top_alerts_map_byhour/3_2_32.png)"
      ],
      "id": "6b81f795",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the plots showing 6am and 8 pm, we can say that is the night hours seeing more constructions on road.\n",
        "\n",
        "The code for app2:"
      ],
      "id": "1171732e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def print_file_contents(file_path):\n",
        "    \"\"\"Print contents of a file.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            content = f.read()\n",
        "            print(\"```python\")\n",
        "            print(content)\n",
        "            print(\"```\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"```python\")\n",
        "        print(f\"Error: File '{file_path}' not found\")\n",
        "        print(\"```\")\n",
        "    except Exception as e:\n",
        "        print(\"```python\") \n",
        "        print(f\"Error reading file: {e}\")\n",
        "        print(\"```\")\n",
        "\n",
        "print_file_contents(\"C:/Users/RedthinkerDantler/Documents/GitHub/DPPP2/PS6_SitongGuo/top_alerts_map_byhour/app2/app2.py\") # Change accordingly"
      ],
      "id": "7192e547",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# App #3: Top Location by Alert Type and Hour Dashboard (20 points){-}\n",
        "\n",
        "1. \n",
        "a. \n",
        "Yes, this way improves efficiency in processing data, also maintain certain granularity in time window analyses. Dividing a day into major components characterize the pattern of traffic situations while avoid going into overly details and confusing the user. \n",
        "\n",
        "b. "
      ],
      "id": "9a610895"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ranged_data = collapsed_data\n",
        "ranged_data['hour'] = pd.to_datetime(ranged_data['hour'], format='%H:%M')\n",
        "\n",
        "sixnine_data = ranged_data[ranged_data['hour'].dt.hour.between(6, 9)]"
      ],
      "id": "0df2f42f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sixnine_data = sixnine_data[\n",
        "    (sixnine_data[\"updated_type\"] == \"Jam\") &\n",
        "    (sixnine_data[\"updated_subtype\"] == \"Heavy Traffic\") \n",
        "]\n",
        "\n",
        "sixnine_data = sixnine_data.groupby(['latitude', 'longitude'], as_index=False)['alert_count'].sum()\n",
        "\n",
        "top_10_location3 = sixnine_data.nlargest(10, 'alert_count')"
      ],
      "id": "533d10c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "scatter_plot3 = alt.Chart(top_10_location3).mark_circle().encode(\n",
        "    longitude='longitude:Q',\n",
        "    latitude='latitude:Q',\n",
        "    size=alt.Size('alert_count:Q', scale=alt.Scale(range=[50, 500])),\n",
        "    color=alt.Color('alert_count:Q', scale=alt.Scale(scheme='reds')),\n",
        "    tooltip=['latitude', 'longitude', 'alert_count']\n",
        ").properties(\n",
        "    width=800,\n",
        "    height=600,\n",
        "    title=\"Top 10 Jam - Heavy Traffic Alerts Between 6 AM and 9 AM\"\n",
        ")\n",
        "\n",
        "# Set axis domains for alignment\n",
        "lat_min, lat_max = top_10_location3['latitude'].min() - 0.01, top_10_location3['latitude'].max() + 0.01\n",
        "lon_min, lon_max = top_10_location3['longitude'].min() - 0.01, top_10_location3['longitude'].max() + 0.01\n",
        "\n",
        "# Combine the layers!!\n",
        "rangeplot = (map_layer + scatter_plot3).configure_view(\n",
        "    stroke=None\n",
        ").configure_title(\n",
        "    fontSize=16,\n",
        "    anchor=\"start\"\n",
        ").encode(\n",
        "    x=alt.X('longitude:Q', scale=alt.Scale(domain=[lon_min, lon_max]), title=\"Longitude\"),\n",
        "    y=alt.Y('latitude:Q', scale=alt.Scale(domain=[lat_min, lat_max]), title=\"Latitude\")\n",
        ").properties(\n",
        "    width=800,\n",
        "    height=600\n",
        ")\n",
        "rangeplot.show()"
      ],
      "id": "99f72ab0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. \n",
        "\n",
        "a. \n",
        "\n",
        "\n",
        "b. \n",
        "    \n",
        "3. \n",
        "\n",
        "a. \n",
        "    \n",
        "\n",
        "b. \n",
        "\n",
        "\n",
        "c. \n",
        "\n",
        "\n",
        "d.\n",
        "1, Add a \"Time Period\" Categorization (Morning vs Afternoon). Categorize data based on the time of day and color the points. This is by extracting the hour and classifying them into morning and afternoon: Morning 6AM - 12PM, Afternoon: 1PM - 6PM. A column to tell that for each case.\n",
        "2, Add a Legend for Size and Time Period.\n",
        "3, Use alt.shape() to assign different marks for the two categories."
      ],
      "id": "33e8dbc2"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "e:\\anaconda3\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}